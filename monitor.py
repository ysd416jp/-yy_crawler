import os, json, base64, re, hashlib
import gspread
import google.generativeai as genai
from google.oauth2.service_account import Credentials
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote

# --- æ¤œç´¢URLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæ—¢çŸ¥ã‚µã‚¤ãƒˆã¯Geminiä¸è¦ï¼‰ ---
# ã‚­ãƒ¼: æ—¥æœ¬èªžåãƒ»è‹±èªžåãƒ»ç•¥ç§°ãªã©è¤‡æ•°ç™»éŒ²ã—ã¦æŸ”è»Ÿã«ãƒžãƒƒãƒ
SEARCH_TEMPLATES = {
    # ã‚°ãƒ«ãƒ¡
    "é£Ÿã¹ãƒ­ã‚°":           "https://tabelog.com/keywords/{word}/kwdLst/",
    "tabelog":            "https://tabelog.com/keywords/{word}/kwdLst/",
    "ãƒ›ãƒƒãƒˆãƒšãƒƒãƒ‘ãƒ¼ã‚°ãƒ«ãƒ¡": "https://www.hotpepper.jp/CSP/psh/rstLst/00/?keyword={word}",
    "ãƒ›ãƒƒãƒˆãƒšãƒƒãƒ‘ãƒ¼":      "https://www.hotpepper.jp/CSP/psh/rstLst/00/?keyword={word}",
    "hotpepper":          "https://www.hotpepper.jp/CSP/psh/rstLst/00/?keyword={word}",
    "ãã‚‹ãªã³":           "https://r.gnavi.co.jp/eki/result/?freeword={word}",
    "gnavi":              "https://r.gnavi.co.jp/eki/result/?freeword={word}",
    "retty":              "https://retty.me/search/?keyword={word}",
    # æ—…è¡Œãƒ»å®¿æ³Š
    "jalan":              "https://www.jalan.net/uw/uwp3200/uww3201init.do?keyword={word}",
    "ã˜ã‚ƒã‚‰ã‚“":           "https://www.jalan.net/uw/uwp3200/uww3201init.do?keyword={word}",
    "æ¥½å¤©ãƒˆãƒ©ãƒ™ãƒ«":        "https://search.travel.rakuten.co.jp/ds/hotellist/search?f_free_word={word}",
    "rakuten travel":     "https://search.travel.rakuten.co.jp/ds/hotellist/search?f_free_word={word}",
    "booking.com":        "https://www.booking.com/searchresults.html?ss={word}",
    "booking":            "https://www.booking.com/searchresults.html?ss={word}",
    # æ±‚äºº
    "indeed":             "https://jp.indeed.com/jobs?q={word}",
    "townwork":           "https://townwork.net/joSrchRsltList/?fw={word}",
    "ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯":        "https://townwork.net/joSrchRsltList/?fw={word}",
    "ãƒªã‚¯ãƒŠãƒ“next":       "https://next.rikunabi.com/search/?freeWordKey={word}",
    "ãƒžã‚¤ãƒŠãƒ“è»¢è·":        "https://tenshoku.mynavi.jp/list/kw{word}/",
    "doda":               "https://doda.jp/keyword/{word}/",
    # SNSãƒ»æ¤œç´¢
    "x":                  "https://x.com/search?q={word}",
    "twitter":            "https://x.com/search?q={word}",
    "youtube":            "https://www.youtube.com/results?search_query={word}",
    "google":             "https://www.google.com/search?q={word}",
    "bing":               "https://www.bing.com/search?q={word}",
    # ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°
    "amazon":             "https://www.amazon.co.jp/s?k={word}",
    "ã‚¢ãƒžã‚¾ãƒ³":           "https://www.amazon.co.jp/s?k={word}",
    "æ¥½å¤©å¸‚å ´":           "https://search.rakuten.co.jp/search/mall/{word}/",
    "rakuten":            "https://search.rakuten.co.jp/search/mall/{word}/",
    "ãƒ¡ãƒ«ã‚«ãƒª":           "https://jp.mercari.com/search?keyword={word}",
    "mercari":            "https://jp.mercari.com/search?keyword={word}",
    "yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°":  "https://shopping.yahoo.co.jp/search?p={word}",
    "yahooã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°":   "https://shopping.yahoo.co.jp/search?p={word}",
    # ä¸å‹•ç”£
    "suumo":              "https://suumo.jp/jj/common/ichiran/JJ010FJ001/?fw={word}",
    "ã‚¹ãƒ¼ãƒ¢":             "https://suumo.jp/jj/common/ichiran/JJ010FJ001/?fw={word}",
    "homes":              "https://www.homes.co.jp/chintai/theme/keyword={word}/",
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹
    "yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":      "https://news.yahoo.co.jp/search?p={word}",
    "yahooãƒ‹ãƒ¥ãƒ¼ã‚¹":       "https://news.yahoo.co.jp/search?p={word}",
    "nhk":                "https://www3.nhk.or.jp/news/json/search/2.0/search.json?q={word}",
}

# --- è»½å¾®å¤‰æ›´ã®é–¾å€¤ (å¤§è¦æ¨¡ã‚µã‚¤ãƒˆã®ã¿é©ç”¨) ---
MIN_CHANGE_CHARS = 50
MIN_CHANGE_RATIO = 0.05
# ãƒ†ã‚­ã‚¹ãƒˆé‡ãŒã“ã®å€¤ä»¥ä¸‹ãªã‚‰è»½å¾®å¤‰æ›´ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç„¡åŠ¹ã«ã™ã‚‹
# (ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—ç­‰ã€ãƒ†ã‚­ã‚¹ãƒˆé‡ãŒå°‘ãªã„ãŒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå…¥ã‚Œæ›¿ã‚ã‚‹ã‚‚ã®)
SMALL_PAGE_THRESHOLD = 5000


def get_credentials():
    """GitHub Secret / ç’°å¢ƒå¤‰æ•°ã‹ã‚‰GCPèªè¨¼æƒ…å ±ã‚’å–å¾—"""
    raw_val = os.environ.get("GCP_JSON") or os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
    if not raw_val:
        raise RuntimeError("GCP_JSON ãŒæœªè¨­å®š")

    raw_stripped = raw_val.strip()

    if raw_stripped.startswith('{'):
        # JSONå½¢å¼ã®å ´åˆã€ãã®ã¾ã¾ãƒ‘ãƒ¼ã‚¹ï¼ˆç©ºç™½ã‚’æ¶ˆã•ãªã„ï¼‰
        creds_info = json.loads(raw_stripped)
    else:
        # Base64å½¢å¼ã®å ´åˆ
        cleaned_val = re.sub(r'[\s\n]', '', raw_val)
        missing_padding = len(cleaned_val) % 4
        if missing_padding:
            cleaned_val += '=' * (4 - missing_padding)
        creds_info = json.loads(base64.b64decode(cleaned_val).decode('utf-8'))

    # private_keyã®æ”¹è¡Œä¿®å¾©
    if 'private_key' in creds_info:
        pk = creds_info['private_key']
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸ \n ã‚’å®Ÿéš›ã®æ”¹è¡Œã«å¤‰æ›
        if '\\n' in pk:
            pk = pk.replace('\\n', '\n')
        # æœ«å°¾ã«æ”¹è¡ŒãŒãªã‘ã‚Œã°è¿½åŠ 
        if not pk.endswith('\n'):
            pk += '\n'
        creds_info['private_key'] = pk

    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]
    return Credentials.from_service_account_info(creds_info, scopes=scopes)


def send_line_notification(message):
    """LINE Messaging APIã§ãƒ—ãƒƒã‚·ãƒ¥é€šçŸ¥ã‚’é€ã‚‹"""
    token = os.environ.get("LINE_CHANNEL_TOKEN")
    user_id = os.environ.get("LINE_USER_ID")
    if not token or not user_id:
        print("LINEé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: TOKEN/USER_IDãŒæœªè¨­å®š")
        return

    resp = requests.post(
        "https://api.line.me/v2/bot/message/push",
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}",
        },
        json={
            "to": user_id,
            "messages": [{"type": "text", "text": message}],
        },
    )
    if resp.status_code == 200:
        print(f"LINEé€šçŸ¥é€ä¿¡OK")
    else:
        print(f"LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {resp.status_code} {resp.text}")


def extract_body_text(html):
    """HTMLã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã ã‘æŠ½å‡ºï¼ˆãƒŽã‚¤ã‚ºé™¤åŽ»ï¼‰"""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe"]):
        tag.decompose()
    return soup.get_text(separator="\n", strip=True)


def get_col_index(headers, name):
    """ãƒ˜ãƒƒãƒ€ãƒ¼åã‹ã‚‰1-basedåˆ—ç•ªå·ã‚’å–å¾—"""
    try:
        return headers.index(name) + 1
    except ValueError:
        return None


def check_site_update(sheet, row_index, row, col_map):
    """ã‚µã‚¤ãƒˆæ›´æ–°ãƒã‚§ãƒƒã‚¯ã€‚è»½å¾®å¤‰æ›´ã¯ã‚¹ã‚­ãƒƒãƒ—ã€é–¾å€¤è¶…ãˆãŸã‚‰LINEé€šçŸ¥"""
    url = str(row.get('url', '')).strip()
    if not url.startswith('http'):
        print(f"  è¡Œ{row_index}: URLãŒæœªè¨­å®šã€ã‚¹ã‚­ãƒƒãƒ—")
        return

    try:
        resp = requests.get(url, timeout=15, headers={"User-Agent": "web-watcher/1.0"})
        resp.raise_for_status()
    except Exception as e:
        print(f"  è¡Œ{row_index}: HTMLå–å¾—å¤±æ•— ({e})")
        return

    current_text = extract_body_text(resp.text)
    current_hash = hashlib.sha256(current_text.encode()).hexdigest()

    prev_hash = str(row.get('prev_hash', '')).strip()

    col_prev_hash = col_map.get('prev_hash')
    col_prev_len = col_map.get('prev_len')

    if not prev_hash:
        if col_prev_hash:
            sheet.update_cell(row_index, col_prev_hash, current_hash)
        if col_prev_len:
            sheet.update_cell(row_index, col_prev_len, str(len(current_text)))
        print(f"  è¡Œ{row_index}: åˆå›žãƒã‚§ãƒƒã‚¯ã€ãƒãƒƒã‚·ãƒ¥ä¿å­˜")
        return

    if current_hash == prev_hash:
        print(f"  è¡Œ{row_index}: å¤‰æ›´ãªã—")
        return

    # --- å·®åˆ†é‡ã‚’è¨ˆç®— ---
    prev_len_str = str(row.get('prev_len', '')).strip()
    current_len = len(current_text)
    change_chars = abs(current_len - (int(prev_len_str) if prev_len_str.isdigit() else 0))

    # å¤§ãã„ãƒšãƒ¼ã‚¸ã®ã¿è»½å¾®å¤‰æ›´ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
    # å°ã•ã„ãƒšãƒ¼ã‚¸ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãƒˆãƒƒãƒ—ç­‰ï¼‰ã¯ãƒãƒƒã‚·ãƒ¥å¤‰åŒ–ã§å³é€šçŸ¥
    if prev_len_str and prev_len_str.isdigit():
        prev_len = int(prev_len_str)
        if prev_len > SMALL_PAGE_THRESHOLD:
            change_ratio = change_chars / max(prev_len, 1)
            if change_chars < MIN_CHANGE_CHARS and change_ratio < MIN_CHANGE_RATIO:
                print(f"  è¡Œ{row_index}: è»½å¾®å¤‰æ›´ï¼ˆ{change_chars}æ–‡å­—, {change_ratio:.1%}ï¼‰ã‚¹ã‚­ãƒƒãƒ—")
                if col_prev_hash:
                    sheet.update_cell(row_index, col_prev_hash, current_hash)
                if col_prev_len:
                    sheet.update_cell(row_index, col_prev_len, str(current_len))
                return

    # --- é€šçŸ¥ ---
    word = str(row.get('word', ''))
    memo = str(row.get('memo', ''))
    if memo == "HPæ›´æ–°":
        label = url
    else:
        label = f"{word}ï¼ˆ{memo}ï¼‰"
    msg = f"ðŸ”” ã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥\n{label}\n{url}"
    send_line_notification(msg)
    print(f"  è¡Œ{row_index}: æ›´æ–°æ¤œçŸ¥ â†’ LINEé€šçŸ¥")

    if col_prev_hash:
        sheet.update_cell(row_index, col_prev_hash, current_hash)
    if col_prev_len:
        sheet.update_cell(row_index, col_prev_len, str(current_len))


def find_template(memo):
    """ãƒ¡ãƒ¢ã‹ã‚‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ¤œç´¢ï¼ˆå®Œå…¨ä¸€è‡´â†’éƒ¨åˆ†ä¸€è‡´ï¼‰"""
    memo_clean = memo.strip().lower()
    # å®Œå…¨ä¸€è‡´
    if memo_clean in SEARCH_TEMPLATES:
        return SEARCH_TEMPLATES[memo_clean]
    # éƒ¨åˆ†ä¸€è‡´ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚­ãƒ¼ãŒãƒ¡ãƒ¢ã«å«ã¾ã‚Œã‚‹ or ãƒ¡ãƒ¢ãŒã‚­ãƒ¼ã«å«ã¾ã‚Œã‚‹ï¼‰
    for key, tmpl in SEARCH_TEMPLATES.items():
        if key in memo_clean or memo_clean in key:
            return tmpl
    return None


def generate_search_url(sheet, row_index, row, gemini_model, col_map):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢URLç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå„ªå…ˆã€æœªçŸ¥ã‚µã‚¤ãƒˆã¯Geminiï¼‰"""
    url_cell = str(row.get('url', '')).strip()
    if url_cell.startswith('http'):
        return  # æ—¢ã«URLã‚ã‚Š

    word = str(row.get('word', '')).strip()
    memo = str(row.get('memo', '')).strip()
    col_url = col_map.get('url', 2)

    if not word:
        return

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã‚ã‚‹ã‚µã‚¤ãƒˆã¯ãã‚Œã‚’ä½¿ã†
    tmpl = find_template(memo)
    if tmpl:
        new_url = tmpl.format(word=quote(word))
        sheet.update_cell(row_index, col_url, new_url)
        print(f"  è¡Œ{row_index}: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆURLç”Ÿæˆ â†’ {new_url}")
        return

    # æœªçŸ¥ã‚µã‚¤ãƒˆã¯Geminiã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not gemini_model:
        print(f"  è¡Œ{row_index}: Geminiæœªè¨­å®šã€ã‚¹ã‚­ãƒƒãƒ—")
        return

    try:
        prompt = (
            f"ã€Œ{memo}ã€ã®ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã§ã€Œ{word}ã€ã‚’æ¤œç´¢ã—ãŸçµæžœãƒšãƒ¼ã‚¸ã®URLã‚’"
            f"1ã¤ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚URLã®ã¿ã‚’å‡ºåŠ›ã—ã€èª¬æ˜Žã¯ä¸è¦ã§ã™ã€‚"
        )
        res = gemini_model.generate_content(prompt)
        new_url = res.text.strip()
        sheet.update_cell(row_index, col_url, new_url)
        print(f"  è¡Œ{row_index}: Gemini URLç”Ÿæˆ â†’ {new_url}")
    except Exception as e:
        print(f"  è¡Œ{row_index}: Geminiç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")


def should_run_now(row, current_hour):
    """é »åº¦è¨­å®šã«åŸºã¥ã„ã¦ä»Šå®Ÿè¡Œã™ã¹ãã‹ã‚’åˆ¤å®š"""
    freq_key = 'count' if 'count' in row else 'freq'
    try:
        freq = int(row.get(freq_key, 1))
    except (ValueError, TypeError):
        freq = 1
    if freq <= 0:
        freq = 1
    return (current_hour % freq) == 0


def main():
    print("--- å‡¦ç†é–‹å§‹ ---")

    try:
        creds = get_credentials()
        client = gspread.authorize(creds)
        sheet = client.open_by_key("1wSfyGreLH_lb7vR_vpmuJ3rAndtMNvMDQbv2ZlPVxUE").sheet1
        print("èªè¨¼æˆåŠŸ")

        # ãƒ˜ãƒƒãƒ€ãƒ¼å–å¾— & åˆ—ãƒžãƒƒãƒ—ä½œæˆ
        headers = sheet.row_values(1)
        col_map = {h: i + 1 for i, h in enumerate(headers)}
        print(f"ãƒ˜ãƒƒãƒ€ãƒ¼: {headers}")

        # prev_hash, prev_lenãŒãªã‘ã‚Œã°è‡ªå‹•è¿½åŠ 
        if "prev_hash" not in col_map:
            idx = len(headers) + 1
            sheet.update_cell(1, idx, "prev_hash")
            col_map["prev_hash"] = idx
            headers.append("prev_hash")
            print("ãƒ˜ãƒƒãƒ€ãƒ¼ã« prev_hash ã‚’è¿½åŠ ")
        if "prev_len" not in col_map:
            idx = len(headers) + 1
            sheet.update_cell(1, idx, "prev_len")
            col_map["prev_len"] = idx
            headers.append("prev_len")
            print("ãƒ˜ãƒƒãƒ€ãƒ¼ã« prev_len ã‚’è¿½åŠ ")

        # Geminiãƒ¢ãƒ‡ãƒ«ï¼ˆã‚­ãƒ¼ãŒã‚ã‚Œã°ï¼‰
        gemini_key = os.environ.get("GEMINI_API_KEY")
        gemini_model = None
        if gemini_key:
            genai.configure(api_key=gemini_key)
            gemini_model = genai.GenerativeModel('gemini-2.5-flash')

        # ç¾åœ¨ã®æ™‚åˆ»ï¼ˆUTCï¼‰
        from datetime import datetime, timezone
        current_hour = datetime.now(timezone.utc).hour

        rows = sheet.get_all_records()
        for i, row in enumerate(rows, start=2):
            memo = str(row.get('memo', '')).strip()

            # URLæœªç”Ÿæˆã®æ¤œç´¢ç›£è¦–ã¯é »åº¦ã«é–¢ä¿‚ãªãå¸¸ã«ç”Ÿæˆã‚’è©¦ã¿ã‚‹
            if memo != "HPæ›´æ–°":
                url_cell = str(row.get('url', '')).strip()
                if not url_cell.startswith('http'):
                    generate_search_url(sheet, i, row, gemini_model, col_map)
                    continue

            # é »åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆURLç”Ÿæˆæ¸ˆã¿ã®ç›£è¦–ãƒ»HPæ›´æ–°ã®ã¿ï¼‰
            if not should_run_now(row, current_hour):
                print(f"  è¡Œ{i}: é »åº¦ã‚¹ã‚­ãƒƒãƒ—")
                continue

            if memo == "HPæ›´æ–°":
                check_site_update(sheet, i, row, col_map)
            else:
                # URLç”Ÿæˆæ¸ˆã¿ãªã‚‰æ›´æ–°ãƒã‚§ãƒƒã‚¯
                check_site_update(sheet, i, row, col_map)

        print("--- å…¨å‡¦ç†å®Œäº† ---")

    except Exception as e:
        print(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
